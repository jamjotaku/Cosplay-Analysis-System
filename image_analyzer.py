import os
import glob
from PIL import Image
import torch
from transformers import CLIPProcessor, CLIPModel

# --- è¨­å®š ---
IMAGE_DIR = 'downloaded_images'
# åˆ¤å®šã•ã›ãŸã„æ§‹å›³ã®ãƒªã‚¹ãƒˆï¼ˆè‹±èªã§å®šç¾©ã—ã¾ã™ï¼‰
COMPOSITION_LABELS = [
    "a close-up photo of a face",       # é¡”ã®ã‚¢ãƒƒãƒ—
    "a bust-up portrait of a person",   # ãƒã‚¹ãƒˆã‚¢ãƒƒãƒ—ï¼ˆèƒ¸ã‹ã‚‰ä¸Šï¼‰
    "a full-body photo of a person",    # å…¨èº«
    "a photo of scenery or objects"     # é¢¨æ™¯ã‚„å°é“å…·ï¼ˆäººç‰©ãƒ¡ã‚¤ãƒ³ã˜ã‚ƒãªã„ï¼‰
]
# ã‚ã‹ã‚Šã‚„ã™ã„è¡¨ç¤ºç”¨ã®æ—¥æœ¬èªãƒ©ãƒ™ãƒ«
LABEL_MAP = {
    0: "é¡”ã‚¢ãƒƒãƒ— (Face Close-up)",
    1: "ãƒã‚¹ãƒˆã‚¢ãƒƒãƒ— (Bust-up)",
    2: "å…¨èº« (Full Body)",
    3: "é¢¨æ™¯ãƒ»ãã®ä»– (Scenery/Others)"
}

# --- ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ï¼ˆåˆå›ã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰ ---
print("ğŸ¤– AIãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­... (åˆå›ã¯æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™)")
model_id = "openai/clip-vit-base-patch32"
# CPUã§ã®å®Ÿè¡Œã‚’æ˜ç¤º
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"   Running on: {device}")

model = CLIPModel.from_pretrained(model_id).to(device)
processor = CLIPProcessor.from_pretrained(model_id)
print("âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼")

def get_latest_image(directory):
    """ æŒ‡å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã§æœ€ã‚‚æ–°ã—ã„ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾— """
    list_of_files = glob.glob(os.path.join(directory, '*.jpg'))
    if not list_of_files:
        return None
    latest_file = max(list_of_files, key=os.path.getctime)
    return latest_file

def analyze_composition(image_path):
    """ ç”»åƒã®æ§‹å›³ã‚’åˆ¤å®šã™ã‚‹ """
    print(f"\nğŸ” Analyzing Image: {image_path}")
    try:
        image = Image.open(image_path)
        
        # ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã§ãã‚‹å½¢å¼ã«å¤‰æ›
        inputs = processor(
            text=COMPOSITION_LABELS,
            images=image,
            return_tensors="pt",
            padding=True
        ).to(device)

        # æ¨è«–å®Ÿè¡Œ
        with torch.no_grad(): # å­¦ç¿’ã—ãªã„ã®ã§ãƒ¡ãƒ¢ãƒªç¯€ç´„
            outputs = model(**inputs)
        
        # ç”»åƒã¨å„ãƒ†ã‚­ã‚¹ãƒˆã®é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã‚’å–å¾—
        logits_per_image = outputs.logits_per_image
        # ã‚¹ã‚³ã‚¢ã‚’ç¢ºç‡ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ï¼‰ã«å¤‰æ› (softmax)
        probs = logits_per_image.softmax(dim=1)
        
        # çµæœã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›
        scores = probs.tolist()[0]
        
        # ã‚¹ã‚³ã‚¢ãŒé«˜ã„é †ã«ä¸¦ã¹æ›¿ãˆ
        results = []
        for i, score in enumerate(scores):
            results.append((i, score * 100))
        results.sort(key=lambda x: x[1], reverse=True)

        # --- çµæœè¡¨ç¤º ---
        print("\n" + "ğŸ¨" * 20)
        print("ğŸ“Š AIæ§‹å›³åˆ¤å®šçµæœ")
        print("ğŸ¨" * 20)
        
        # æœ€ã‚‚å¯èƒ½æ€§ãŒé«˜ã„åˆ¤å®š
        top_label_idx = results[0][0]
        top_score = results[0][1]
        print(f"ğŸ† åˆ¤å®š: ã€ {LABEL_MAP[top_label_idx]} ã€‘ (ç¢ºä¿¡åº¦: {top_score:.1f}%)")
        print("-" * 30)
        
        print("è©³ç´°ã‚¹ã‚³ã‚¢:")
        for label_idx, score in results:
             print(f"  - {LABEL_MAP[label_idx]:<20}: {score:.1f}%")
        print("ğŸ¨" * 20 + "\n")

    except Exception as e:
        print(f"âŒ Error analyzing image: {e}")

if __name__ == "__main__":
    # æœ€æ–°ã®ç”»åƒã‚’å–å¾—ã—ã¦è§£æ
    latest_img = get_latest_image(IMAGE_DIR)
    if latest_img:
        analyze_composition(latest_img)
    else:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: '{IMAGE_DIR}' ãƒ•ã‚©ãƒ«ãƒ€ã«ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        print("å…ˆã« fetch_tweet_data.py ã‚’å®Ÿè¡Œã—ã¦ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")