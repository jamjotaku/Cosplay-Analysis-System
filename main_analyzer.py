import sys
import asyncio
import json
import re
import os
import torch
import random
import cv2
import numpy as np
from datetime import datetime
from PIL import Image
from playwright.async_api import async_playwright
from transformers import CLIPProcessor, CLIPModel

# --- è¨­å®š ---
AUTH_FILE = 'auth.json'
SAVE_DIR = 'static/images'
DB_FILE = 'analysis_db.json'
AI_MODEL_ID = "openai/clip-vit-base-patch32"

# ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
os.makedirs(SAVE_DIR, exist_ok=True)

# --- 1. æ§‹å›³ (Composition) ---
COMPOSITION_DEFINITIONS = {
    "Face Close-up": ["close-up of a face", "face shot", "portrait focusing on face"],
    "Bust-up": ["upper body portrait", "bust-up shot", "waist up photo"],
    "Full Body": ["full body shot", "whole body showing shoes", "standing pose"],
    "Object/Scenery": ["no humans", "scenery only", "objects only"]
}

# --- ðŸ¤– AIãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ– ---
print("ðŸ¤– Loading AI Model...")
device = "cuda" if torch.cuda.is_available() else "cpu"
model = CLIPModel.from_pretrained(AI_MODEL_ID).to(device)
processor = CLIPProcessor.from_pretrained(AI_MODEL_ID)

def extract_number(text):
    """ æ•°å€¤æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ï¼ˆM/Kèª¤çˆ†é˜²æ­¢ç‰ˆï¼‰ """
    if not text: return 0
    clean = text.replace(',', '').strip()
    upper = clean.upper()
    mul = 1
    if 'K' in upper:
        if 'BOOKMARK' not in upper and 'LIKES' not in upper: mul = 1000
    elif 'M' in upper:
        if 'BOOKMARK' not in upper and 'IMAGE' not in upper and 'COMMENT' not in upper: mul = 1000000
    match = re.search(r'(\d+(?:\.\d+)?)', clean)
    return int(float(match.group(1)) * mul) if match else 0

def analyze_skin_ratio(img_path):
    """ â˜…è‚Œè‰²ãƒ”ã‚¯ã‚»ãƒ«çŽ‡ã‚’è¨ˆç®—ã™ã‚‹ (0.0 - 100.0) """
    try:
        img = cv2.imread(img_path)
        if img is None: return 0.0
        
        # HSVè‰²ç©ºé–“ã«å¤‰æ›
        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
        
        # è‚Œè‰²ã®ç¯„å›²å®šç¾© (ä¸€èˆ¬çš„ãªè‚Œè‰²)
        lower_skin = np.array([0, 20, 70], dtype=np.uint8)
        upper_skin = np.array([20, 255, 255], dtype=np.uint8)
        
        # è‚Œè‰²ãƒžã‚¹ã‚¯ã‚’ä½œæˆ
        mask = cv2.inRange(hsv, lower_skin, upper_skin)
        
        # è‚Œè‰²ãƒ”ã‚¯ã‚»ãƒ«æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        skin_pixels = cv2.countNonZero(mask)
        total_pixels = img.shape[0] * img.shape[1]
        
        return round((skin_pixels / total_pixels) * 100, 2)
    except Exception as e:
        print(f"âš ï¸ Skin analysis failed: {e}")
        return 0.0

def analyze_color_and_brightness(img_path):
    """ ç”»åƒã®ä¸»è¦è‰²ã¨æ˜Žã‚‹ã•ã‚’è§£æžã™ã‚‹ """
    try:
        img = Image.open(img_path).convert("RGB")
        gray_img = img.convert("L")
        brightness_val = gray_img.resize((1, 1)).getpixel((0, 0))
        brightness_tag = "Normal"
        if brightness_val > 170: brightness_tag = "Bright"
        elif brightness_val < 85: brightness_tag = "Dark"
        img_small = img.resize((150, 150))
        result = img_small.quantize(colors=5, method=2)
        dominant_color = result.getpalette()[:3]
        hex_color = '#{:02x}{:02x}{:02x}'.format(*dominant_color)
        return hex_color, brightness_tag
    except:
        return "#000000", "Unknown"

def predict_composition(pil_img):
    """ æ§‹å›³åˆ¤å®š (CLIP) """
    labels = []
    flattened_prompts = []
    for label, prompts in COMPOSITION_DEFINITIONS.items():
        for p in prompts:
            labels.append(label)
            flattened_prompts.append(f"a photo of {p}")

    inputs = processor(text=flattened_prompts, images=pil_img, return_tensors="pt", padding=True).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
    
    logits = outputs.logits_per_image.softmax(dim=1)[0]
    scores = {key: [] for key in COMPOSITION_DEFINITIONS.keys()}
    for i, score in enumerate(logits):
        scores[labels[i]].append(score.item())
    
    avg_scores = {k: sum(v)/len(v) for k, v in scores.items()}
    best = max(avg_scores, key=avg_scores.get)
    return best, round(avg_scores[best] * 100, 1)

def get_tweet_time(tweet_id):
    """ â˜…Tweet IDã‹ã‚‰æŠ•ç¨¿æ—¥æ™‚ã‚’é€†ç®— (Snowflake) """
    try:
        # Twitter Epoch (1288834974657) ã‚’åŠ ç®—
        t_ms = (int(tweet_id) >> 22) + 1288834974657
        return datetime.fromtimestamp(t_ms / 1000.0).isoformat()
    except:
        return None

def normalize_url(url):
    """ URLæ­£è¦åŒ– """
    match = re.search(r'(https?://(?:x|twitter)\.com/[a-zA-Z0-9_]+/status/\d+)', url)
    if match:
        return match.group(1)
    return url.split('?')[0]

async def run_analysis(raw_url):
    tweet_url = normalize_url(raw_url)
    tweet_id = tweet_url.split('/')[-1]
    
    # --- ã‚¹ã‚­ãƒƒãƒ—æ©Ÿèƒ½ (è‚Œè‰²ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã‚‚å†å‡¦ç†) ---
    if os.path.exists(DB_FILE):
        try:
            with open(DB_FILE, 'r', encoding='utf-8') as f:
                db = json.load(f)
                entry = next((e for e in db if e.get('tweet_id') == tweet_id), None)
                
                # ã‚¨ãƒ³ãƒˆãƒªãŒå­˜åœ¨ã—ã€ã‹ã¤ç”»åƒãŒã‚ã‚Šã€ã‹ã¤è‚Œè‰²ãƒ‡ãƒ¼ã‚¿(skin_ratio)ã‚‚æŒã£ã¦ã„ã‚‹å ´åˆã®ã¿ã‚¹ã‚­ãƒƒãƒ—
                if entry:
                    # ç”»åƒãŒãªã„ãƒ‡ãƒ¼ã‚¿(å¤±æ•—ãƒ‡ãƒ¼ã‚¿)ã¯å†å–å¾—ã•ã›ã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ãªã„
                    if not entry.get('images'): 
                        pass 
                    # è‚Œè‰²ãƒ‡ãƒ¼ã‚¿å–å¾—æ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    elif 'images' in entry and len(entry['images']) > 0 and 'skin_ratio' in entry['images'][0]:
                        print(f"â© Skip: {tweet_id} (Fully analyzed)")
                        return
        except: pass
    
    # BANå¯¾ç­–ã®å¾…ã¡æ™‚é–“
    await asyncio.sleep(random.uniform(1, 3))

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            storage_state=AUTH_FILE if os.path.exists(AUTH_FILE) else None,
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            locale="en-US"
        )
        page = await context.new_page()

        try:
            print(f"ðŸ“¡ Analyzing: {tweet_url}")
            try:
                await page.goto(tweet_url, wait_until="networkidle", timeout=60000)
                await asyncio.sleep(5)
            except Exception as e:
                print(f"âš ï¸ Load Warning: {e}")

            # --- ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾— (å®Œå…¨ç‰ˆ) ---
            data = {'likes': 0, 'reposts': 0, 'bookmarks': 0, 'views': 0}
            
            # å„ãƒœã‚¿ãƒ³ã®aria-labelã‹ã‚‰æ•°å€¤ã‚’å–å¾—
            targets = [
                ('likes', ['like', 'unlike']), 
                ('reposts', ['retweet', 'unretweet']), 
                ('bookmarks', ['bookmark', 'removeBookmark'])
            ]
            
            for key, ids in targets:
                for tid in ids:
                    btn = await page.query_selector(f'[data-testid="{tid}"]')
                    if btn:
                        aria_label = await btn.get_attribute("aria-label")
                        val = extract_number(aria_label)
                        if val > 0: 
                            data[key] = val
                            break
            
            # ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°
            view_link = await page.query_selector('a[href*="/analytics"]')
            if view_link: 
                aria_label = await view_link.get_attribute("aria-label")
                inner_text = await view_link.inner_text()
                data['views'] = extract_number(aria_label or inner_text)

            # --- ç”»åƒå‡¦ç† & è§£æž ---
            image_results = []
            photo_container = await page.query_selector('[data-testid="tweetPhoto"]')
            
            if photo_container:
                images = await photo_container.query_selector_all('img')
                processed_urls = set()
                
                for i, img in enumerate(images):
                    src = await img.get_attribute('src')
                    if src and 'pbs.twimg.com/media' in src:
                        base_url = src.split('?')[0]
                        fmt = 'png' if 'format=png' in src else 'jpg'
                        high_res = f"{base_url}?format={fmt}&name=orig"
                        
                        if high_res in processed_urls: continue
                        processed_urls.add(high_res)

                        img_path = os.path.join(SAVE_DIR, f"{tweet_id}_{len(image_results)+1}.jpg")
                        
                        img_page = await context.new_page()
                        try:
                            resp = await img_page.goto(high_res)
                            if resp:
                                with open(img_path, "wb") as f: f.write(await resp.body())
                                
                                # === AIè§£æž ===
                                pil_img = Image.open(img_path)
                                
                                # 1. æ§‹å›³
                                comp_cat, comp_conf = predict_composition(pil_img)
                                # 2. è‰²ãƒ»æ˜Žã‚‹ã•
                                hex_color, brightness = analyze_color_and_brightness(img_path)
                                # 3. â˜…è‚Œè‰²çŽ‡
                                skin_ratio = analyze_skin_ratio(img_path)

                                image_results.append({
                                    "path": img_path,
                                    "url": high_res,
                                    "composition": comp_cat,
                                    "confidence": comp_conf,
                                    "color": hex_color,
                                    "brightness": brightness,
                                    "skin_ratio": skin_ratio # â˜…è¿½åŠ 
                                })
                        finally:
                            await img_page.close()

            # å¤±æ•—åˆ¤å®š: ç”»åƒãªã— & ã„ã„ã­0 ã¯ä¿å­˜ã—ãªã„
            if not image_results and data['likes'] == 0:
                print(f"âš ï¸ Analysis Failed (No Data) for {tweet_id}. Skipping Save.")
                return 

            # ä¿å­˜ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
            final_result = {
                "tweet_id": tweet_id,
                "url": tweet_url,
                "timestamp": datetime.now().isoformat(),
                "created_at": get_tweet_time(tweet_id), # â˜…è¿½åŠ : æŠ•ç¨¿æ—¥æ™‚
                "metrics": data,
                "images": image_results,
                "engagement_rate": round((data['likes']/data['views']*100), 2) if data['views'] > 0 else 0,
                "save_rate": round((data['bookmarks']/data['likes']*100), 2) if data['likes'] > 0 else 0
            }

            # DBä¿å­˜
            db = []
            if os.path.exists(DB_FILE):
                try: 
                    with open(DB_FILE, 'r', encoding='utf-8') as f: 
                        db = json.load(f)
                except: db = []
            
            # é‡è¤‡æŽ’é™¤ã—ã¦è¿½è¨˜
            db = [entry for entry in db if entry['tweet_id'] != tweet_id]
            db.append(final_result)
            
            with open(DB_FILE, 'w', encoding='utf-8') as f:
                json.dump(db, f, ensure_ascii=False, indent=2)

            print(f"\nâœ… Complete: {tweet_id} | Skin: {image_results[0]['skin_ratio'] if image_results else 0}%")

        except Exception as e:
            print(f"âŒ Error: {e}")
        finally:
            await browser.close()

if __name__ == "__main__":
    if len(sys.argv) > 1:
        asyncio.run(run_analysis(sys.argv[1]))
    else:
        # ãƒ†ã‚¹ãƒˆç”¨URL
        asyncio.run(run_analysis("https://x.com/snow_sayu_/status/1867910835085148236"))