import sys
import asyncio
import json
import re
import os
import torch
from datetime import datetime
from PIL import Image
from playwright.async_api import async_playwright
from transformers import CLIPProcessor, CLIPModel

# --- è¨­å®š ---
AUTH_FILE = 'auth.json'
# Webã‚¢ãƒ—ãƒªã§è¡¨ç¤ºã™ã‚‹ãŸã‚ã« static ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã—ã¾ã™
SAVE_DIR = 'static/images'
DB_FILE = 'analysis_db.json'
AI_MODEL_ID = "openai/clip-vit-base-patch32"

# ãƒ•ã‚©ãƒ«ãƒ€æº–å‚™
os.makedirs(SAVE_DIR, exist_ok=True)

# --- AIåˆ¤å®šãƒ©ãƒ™ãƒ«å®šç¾© ---

# 1. æ§‹å›³ (Composition)
COMPOSITION_LABELS = [
    "a close-up photo of a face",       # é¡”ã‚¢ãƒƒãƒ—
    "a bust-up portrait of a person",   # ãƒã‚¹ãƒˆã‚¢ãƒƒãƒ—
    "a full-body photo of a person",    # å…¨èº«
    "a photo of scenery or objects"     # é¢¨æ™¯ãƒ»ç‰©
]
LABEL_MAP_COMP = {
    0: "Face Close-up",
    1: "Bust-up",
    2: "Full Body",
    3: "Object/Scenery"
}

# 2. ã‚·ãƒãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ (Situation) â˜…è¿½åŠ æ©Ÿèƒ½
SITUATION_LABELS = [
    "a photo taken in a professional photo studio with lighting", # ã‚¹ã‚¿ã‚¸ã‚ª
    "a photo taken at an outdoor cosplay event or street",        # å±‹å¤–ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆ
    "a mirror selfie taken with a smartphone",                    # è‡ªæ’®ã‚Š
    "a photo taken in a bedroom or home environment"              # å®¶ãƒ»éƒ¨å±‹
]
LABEL_MAP_SIT = {
    0: "Studio",
    1: "Outdoor/Event",
    2: "Selfie",
    3: "Home/Room"
}

# --- AIãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ– (ã‚°ãƒ­ãƒ¼ãƒãƒ«) ---
print("ðŸ¤– Loading AI Model...")
device = "cuda" if torch.cuda.is_available() else "cpu"
model = CLIPModel.from_pretrained(AI_MODEL_ID).to(device)
processor = CLIPProcessor.from_pretrained(AI_MODEL_ID)

def extract_number(text):
    """ æ•°å€¤æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ï¼ˆBookmarksã®'k'èª¤çˆ†é˜²æ­¢ç‰ˆï¼‰ """
    if not text: return 0
    clean = text.replace(',', '').strip()
    mul = 1
    
    upper_text = clean.upper()
    if 'K' in upper_text:
        # "BOOKMARK" ã‚„ "WORK" ãªã©å˜èªžã®ä¸€éƒ¨ã¨ã—ã¦ã®Kã‚’é™¤å¤–
        if 'BOOKMARK' not in upper_text and 'LIKES' not in upper_text:
            mul = 1000
    elif 'M' in upper_text:
        if 'IMAGE' not in upper_text:
            mul = 1000000

    match = re.search(r'(\d+(?:\.\d+)?)', clean)
    return int(float(match.group(1)) * mul) if match else 0

async def run_analysis(tweet_url):
    tweet_id = tweet_url.split('/')[-1].split('?')[0]
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        # è§£æžç²¾åº¦å‘ä¸Šã®ãŸã‚è‹±èªžãƒ­ã‚±ãƒ¼ãƒ«ã§ã‚¢ã‚¯ã‚»ã‚¹
        context = await browser.new_context(
            storage_state=AUTH_FILE if os.path.exists(AUTH_FILE) else None,
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            locale="en-US"
        )
        page = await context.new_page()

        try:
            print(f"ðŸ“¡ Fetching data from X: {tweet_url}")
            await page.goto(tweet_url, wait_until="domcontentloaded")
            await asyncio.sleep(4)

            # --- 1. ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾— ---
            data = {'likes': 0, 'reposts': 0, 'bookmarks': 0, 'views': 0}
            
            # ãƒœã‚¿ãƒ³ã‹ã‚‰æ•°å€¤å–å¾—
            targets = [
                ('likes', ['like', 'unlike']),
                ('reposts', ['retweet', 'unretweet']),
                ('bookmarks', ['bookmark', 'removeBookmark'])
            ]
            for key, ids in targets:
                for tid in ids:
                    btn = await page.query_selector(f'[data-testid="{tid}"]')
                    if btn:
                        label = await btn.get_attribute("aria-label")
                        val = extract_number(label)
                        if val > 0:
                            data[key] = val
                            break
            
            # Viewså–å¾—
            view_link = await page.query_selector('a[href*="/analytics"]')
            if view_link:
                label = await view_link.get_attribute("aria-label") or await view_link.inner_text()
                data['views'] = extract_number(label)

            # --- 2. ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ & ãƒ€ãƒ–ãƒ«AIè§£æž ---
            image_results = []
            photo_container = await page.query_selector('[data-testid="tweetPhoto"]')
            
            if photo_container:
                images = await photo_container.query_selector_all('img')
                processed_urls = set()
                
                for i, img in enumerate(images):
                    src = await img.get_attribute('src')
                    if src and 'pbs.twimg.com/media' in src:
                        # é«˜ç”»è³ªURLç”Ÿæˆ
                        base_url = src.split('?')[0]
                        fmt = 'png' if 'format=png' in src else 'jpg'
                        high_res = f"{base_url}?format={fmt}&name=orig"
                        
                        if high_res in processed_urls: continue
                        processed_urls.add(high_res)

                        img_path = os.path.join(SAVE_DIR, f"{tweet_id}_{len(image_results)+1}.jpg")
                        
                        # ç”»åƒä¿å­˜
                        img_page = await context.new_page()
                        try:
                            resp = await img_page.goto(high_res)
                            if resp:
                                body = await resp.body()
                                with open(img_path, "wb") as f: f.write(body)
                                
                                # === AIè§£æžã‚¹ã‚¿ãƒ¼ãƒˆ ===
                                try:
                                    pil_img = Image.open(img_path)
                                    
                                    # è§£æžA: æ§‹å›³ (Composition)
                                    inputs_comp = processor(text=COMPOSITION_LABELS, images=pil_img, return_tensors="pt", padding=True).to(device)
                                    with torch.no_grad():
                                        outputs_comp = model(**inputs_comp)
                                    probs_comp = outputs_comp.logits_per_image.softmax(dim=1).tolist()[0]
                                    top_comp_idx = probs_comp.index(max(probs_comp))
                                    
                                    # è§£æžB: ã‚·ãƒãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ (Situation) â˜…ã“ã“ãŒè¿½åŠ 
                                    inputs_sit = processor(text=SITUATION_LABELS, images=pil_img, return_tensors="pt", padding=True).to(device)
                                    with torch.no_grad():
                                        outputs_sit = model(**inputs_sit)
                                    probs_sit = outputs_sit.logits_per_image.softmax(dim=1).tolist()[0]
                                    top_sit_idx = probs_sit.index(max(probs_sit))

                                    image_results.append({
                                        "path": img_path,
                                        "url": high_res,
                                        "composition": LABEL_MAP_COMP[top_comp_idx], # æ§‹å›³çµæžœ
                                        "situation": LABEL_MAP_SIT[top_sit_idx],     # ã‚·ãƒãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³çµæžœ
                                        "confidence": round(max(probs_comp) * 100, 1)
                                    })
                                except Exception as ai_e:
                                    print(f"âš ï¸ AI Analysis Failed: {ai_e}")
                        finally:
                            await img_page.close()

            # --- 3. ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ ---
            final_result = {
                "tweet_id": tweet_id,
                "url": tweet_url,
                "timestamp": datetime.now().isoformat(),
                "metrics": data,
                "images": image_results,
                "engagement_rate": round((data['likes']/data['views']*100), 2) if data['views'] > 0 else 0,
                "save_rate": round((data['bookmarks']/data['likes']*100), 2) if data['likes'] > 0 else 0
            }

            # 4. JSON DBæ›´æ–°
            db = []
            if os.path.exists(DB_FILE):
                try:
                    with open(DB_FILE, 'r', encoding='utf-8') as f: db = json.load(f)
                except: db = []
            
            # é‡è¤‡å‰Šé™¤ã—ã¦è¿½åŠ 
            db = [entry for entry in db if entry['tweet_id'] != tweet_id]
            db.append(final_result)
            
            with open(DB_FILE, 'w', encoding='utf-8') as f:
                json.dump(db, f, ensure_ascii=False, indent=2)

            print(f"\nâœ… Analysis Complete!")
            print(f"ðŸ“Š Save Rate: {final_result['save_rate']}%")
            if image_results:
                print(f"ðŸ–¼ï¸ AI Tags: {image_results[0]['composition']} / {image_results[0]['situation']}")

        except Exception as e:
            print(f"âŒ Critical Error: {e}")
        finally:
            await browser.close()

if __name__ == "__main__":
    if len(sys.argv) > 1:
        asyncio.run(run_analysis(sys.argv[1]))
    else:
        # ãƒ†ã‚¹ãƒˆç”¨
        asyncio.run(run_analysis("https://x.com/snow_sayu_/status/1867910835085148236"))