import os
import json
import asyncio
import csv
import threading
import time
import random
from flask import Flask, render_template, request, redirect, url_for
from main_analyzer import run_analysis

app = Flask(__name__)
app.config['DB_FILE'] = 'analysis_db.json'
app.config['UPLOAD_FOLDER'] = 'uploads'
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

def load_data():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰è§£ææ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    if os.path.exists(app.config['DB_FILE']):
        with open(app.config['DB_FILE'], 'r', encoding='utf-8') as f:
            try:
                return json.load(f)
            except:
                return []
    return []

# --- ğŸš€ è§£æãƒŸãƒƒã‚·ãƒ§ãƒ³ãƒ»ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ« (è€ä¹…ä»•æ§˜) ---

def background_batch_analysis(csv_path):
    """
    ã€è€ä¹…ãƒ¬ãƒ¼ã‚¹ä»•æ§˜ã€‘ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ã‚’å›é¿ã—ãªãŒã‚‰å®Œèµ°ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯
   
    """
    urls = []
    try:
        with open(csv_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                url = row.get('Expanded URL') or row.get('URL')
                if url:
                    urls.append(url)
    except Exception as e:
        print(f"âŒ CSVèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        return

    total = len(urls)
    print(f"ğŸš€ è§£æãƒãƒ©ã‚½ãƒ³é–‹å§‹: å…¨ {total} ä»¶")

    for i, url in enumerate(urls):
        current_num = i + 1
        try:
            print(f"ğŸ”„ [{current_num}/{total}] è§£æä¸­: {url}")
            
            # éåŒæœŸé–¢æ•°ã‚’åŒæœŸçš„ã«å®Ÿè¡Œ
            asyncio.run(run_analysis(url))
            
            # --- ğŸ›¡ï¸ ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™(ãƒœãƒƒãƒˆåˆ¤å®š)å›é¿ãƒ­ã‚¸ãƒƒã‚¯ ---
            
            # 1. æ¯å›ã®ã€Œã‚†ã‚‰ãã€å¾…æ©Ÿ (5~12ç§’)
            # äººé–“ãŒæ“ä½œã—ã¦ã„ã‚‹ã‚ˆã†ãªä¸è¦å‰‡ãªé–“éš”ã‚’ä½œã‚‹
            time.sleep(random.uniform(5, 12))
            
            # 2. 50ä»¶ã”ã¨ã®ã€Œå¤§ä¼‘æ†©ã€ (5åˆ†)
            # ã‚µãƒ¼ãƒãƒ¼å´ã®ç›£è¦–ã‚’ãƒªã‚»ãƒƒãƒˆã•ã›ã‚‹ãŸã‚ã®ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³
            if current_num % 50 == 0:
                print(f"â˜• 50ä»¶åˆ°é”ã€‚Xå´ã®ç›£è¦–ã‚’ãã‚‰ã™ãŸã‚5åˆ†é–“ä¼‘æ†©ã—ã¾ã™...")
                time.sleep(300) 
                
        except Exception as e:
            print(f"âš ï¸ ã‚¹ã‚­ãƒƒãƒ— ({url}): {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯Xå´ã®ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’è€ƒæ…®ã—ã€å°‘ã—é•·ã‚ã«ä¼‘ã‚€
            time.sleep(30)
            continue

    print("ğŸ‰ å…¨ã¦ã®è§£æã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

@app.route('/analyze_single', methods=['POST'])
def analyze_single():
    """ç‰¹å®šã®URLã‚’å³åº§ã«1ä»¶è§£æã™ã‚‹ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½"""
    url = request.form.get('url')
    if url:
        asyncio.run(run_analysis(url))
    return redirect(url_for('index'))

@app.route('/upload_csv', methods=['POST'])
def upload_csv():
    """CSVã‚’å—ã‘å–ã‚Šã€ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’åˆ†é›¢ã—ã¦è§£æã‚’é–‹å§‹ã™ã‚‹"""
    file = request.files.get('file')
    if file and file.filename.endswith('.csv'):
        path = os.path.join(app.config['UPLOAD_FOLDER'], file.filename)
        file.save(path)
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§è§£æã‚’ã‚­ãƒƒã‚¯
        threading.Thread(target=background_batch_analysis, args=(path,)).start()
    return redirect(url_for('index'))

# --- ğŸ“Š çµ±è¨ˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ»ãƒ­ã‚¸ãƒƒã‚¯ ---

@app.route('/')
def index():
    data = load_data()
    # æœ€æ–°ã®è§£æçµæœãŒä¸Šã«ãã‚‹ã‚ˆã†ã‚½ãƒ¼ãƒˆ
    data.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
    return render_template('index.html', tweets=data)

@app.route('/stats')
def stats():
    """æ·±æ˜ã‚Šåˆ†æãƒ»ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ»æ§‹å›³åŠ¹ç‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
    data = load_data()
    if not data:
        return render_template('stats.html', stats=None)
    
    stats_data = {
        "total_tweets": len(data),
        "avg_save_rate": 0, "avg_eng_rate": 0,
        "hourly_stats": [0] * 24,
        "scatter_data": [], "skin_scatter": [],
        "comp_efficiency": {}, # æ§‹å›³åˆ¥å¹³å‡ä¿å­˜ç‡
        "raw_tweets": data     # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«ç”¨
    }
    
    h_sums, h_counts, comp_map = [0]*24, [0]*24, {}
    total_s, total_e = 0, 0
    
    for d in data:
        s_rate = d.get('save_rate', 0)
        e_rate = d.get('engagement_rate', 0)
        total_s += s_rate; total_e += e_rate
        
        img = d['images'][0] if d.get('images') else {}
        comp = img.get('composition', 'Unknown')
        
        # æ•£å¸ƒå›³ãƒ‡ãƒ¼ã‚¿
        stats_data['scatter_data'].append({'x': e_rate, 'y': s_rate, 'id': d.get('tweet_id')})
        if 'skin_ratio' in img:
            stats_data['skin_scatter'].append({'x': img['skin_ratio'], 'y': s_rate, 'comp': comp})
            
        # æ§‹å›³é›†è¨ˆ
        if comp not in comp_map: comp_map[comp] = []
        comp_map[comp].append(s_rate)

        # æ™‚é–“å¸¯é›†è¨ˆ
        c_at = d.get('created_at')
        if c_at and 'T' in c_at:
            try:
                hour = int(c_at.split('T')[1].split(':')[0])
                h_sums[hour] += s_rate; h_counts[hour] += 1
            except: pass

    # å„çµ±è¨ˆå€¤ã®ç®—å‡º
    stats_data['avg_save_rate'] = round(total_s / len(data), 2)
    stats_data['avg_eng_rate'] = round(total_e / len(data), 2)
    for h in range(24):
        if h_counts[h] > 0: stats_data['hourly_stats'][h] = round(h_sums[h]/h_counts[h], 2)
    
    # æ§‹å›³åŠ¹ç‡
    stats_data['comp_efficiency'] = {k: round(sum(v)/len(v), 2) for k, v in comp_map.items()}
    
    return render_template('stats.html', stats=stats_data)

@app.route('/clear_db')
def clear_db():
    if os.path.exists(app.config['DB_FILE']):
        os.remove(app.config['DB_FILE'])
    return redirect(url_for('index'))

if __name__ == '__main__':
    app.run(debug=True, port=5000)